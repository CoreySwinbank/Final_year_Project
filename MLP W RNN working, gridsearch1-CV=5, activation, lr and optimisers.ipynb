{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.utils import to_categorical ## specific  one I wanna use\n",
    "from tensorflow.keras import utils\n",
    "#from tensorflow.keras.layers import Input, Dense, Dropout  # Specific ones I wanna use, can just import whole module\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.models import Model\n",
    "import h5py\n",
    "#import tables\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_available = tf.test.is_gpu_available()\n",
    "gpu_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=1\n",
    "y=3\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HDF5 file\n",
    "with h5py.File('DF_event_based.h5', 'r') as hf:\n",
    "    # Access the dataset by name and load it into a pandas dataframe\n",
    "    df_total = pd.DataFrame(hf['dataset_name'][:])\n",
    "\n",
    "with h5py.File('4_event_types_weights.h5', 'r') as f:\n",
    "    df_weights = f['array'][:]\n",
    "\n",
    "with h5py.File('predictions_16pad_128_50epoch_4Layers_SEED10.h5', 'r') as f:\n",
    "    predicted_val = f['val_preds'][:]\n",
    "    predicted_train = f['train_preds'][:]\n",
    "\n",
    "with h5py.File('4_event_types_labels.h5', 'r') as f:\n",
    "    df_labels = f['array'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df's now have no titles, but doesnt matter for the model obvs.\n",
    "df_total = df_total.rename(columns={df_total.columns[0]: 'dataset'})\n",
    "#df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xvars = df_total.drop(['dataset'], axis = 1, inplace = False)   #Need to remove dataset (prediction) column\n",
    "#df_Xvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are [11] variables in this model\n",
      "The dataset has 881096 processes\n",
      "[11]\n"
     ]
    }
   ],
   "source": [
    "x = df_Xvars.values\n",
    "y = df_labels\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(x, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=10)\n",
    "\n",
    "input_shape = [X_train.shape[1]+1]\n",
    "print('There are', input_shape, 'variables in this model')\n",
    "print('The dataset has', len(y), 'processes')\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predicted_train.shape)\n",
    "# print(predicted_val.shape)\n",
    "# print(X_train.shape)\n",
    "# print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(704876, 11)\n",
      "(176220, 11)\n",
      "[0.61670357 0.46361521 0.50274837 ... 0.20900938 0.78310931 0.48962942]\n",
      "[[0.61670357]\n",
      " [0.4636152 ]\n",
      " [0.50274837]\n",
      " ...\n",
      " [0.20900938]\n",
      " [0.7831093 ]\n",
      " [0.48962942]]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack((X_train, predicted_train))\n",
    "X_valid = np.hstack((X_valid, predicted_val))\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_train[:,-1])\n",
    "print(predicted_train)\n",
    "\n",
    "input_shape = [X_train.shape[1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 563900 samples\n",
      "Epoch 1/50\n",
      "563900/563900 [==============================] - 11s 19us/sample - loss: 0.5554 - accuracy: 0.7236\n",
      "Epoch 2/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5419 - accuracy: 0.7336\n",
      "Epoch 3/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5407 - accuracy: 0.7352\n",
      "Epoch 4/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5395 - accuracy: 0.7361\n",
      "Epoch 5/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5392 - accuracy: 0.7358\n",
      "Epoch 6/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5386 - accuracy: 0.7364\n",
      "Epoch 7/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5382 - accuracy: 0.7357\n",
      "Epoch 8/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5376 - accuracy: 0.7360\n",
      "Epoch 9/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5377 - accuracy: 0.7367\n",
      "Epoch 10/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5371 - accuracy: 0.7366\n",
      "Epoch 11/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5370 - accuracy: 0.7374\n",
      "Epoch 12/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5367 - accuracy: 0.7365\n",
      "Epoch 13/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5367 - accuracy: 0.7369\n",
      "Epoch 14/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5362 - accuracy: 0.7369\n",
      "Epoch 15/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5360 - accuracy: 0.7377\n",
      "Epoch 16/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5357 - accuracy: 0.7378\n",
      "Epoch 17/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5359 - accuracy: 0.7366\n",
      "Epoch 18/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5361 - accuracy: 0.7377\n",
      "Epoch 19/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5357 - accuracy: 0.7373\n",
      "Epoch 20/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5353 - accuracy: 0.7375\n",
      "Epoch 21/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5353 - accuracy: 0.7374\n",
      "Epoch 22/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5354 - accuracy: 0.7376\n",
      "Epoch 23/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5354 - accuracy: 0.7375\n",
      "Epoch 24/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5352 - accuracy: 0.7376\n",
      "Epoch 25/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5351 - accuracy: 0.7378\n",
      "Epoch 26/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5353 - accuracy: 0.7370\n",
      "Epoch 27/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5348 - accuracy: 0.7377\n",
      "Epoch 28/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5349 - accuracy: 0.7373\n",
      "Epoch 29/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5347 - accuracy: 0.7376\n",
      "Epoch 30/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5346 - accuracy: 0.7382\n",
      "Epoch 31/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5347 - accuracy: 0.7378\n",
      "Epoch 32/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5347 - accuracy: 0.7376\n",
      "Epoch 33/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5346 - accuracy: 0.7383\n",
      "Epoch 34/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5345 - accuracy: 0.7381\n",
      "Epoch 35/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5341 - accuracy: 0.7391\n",
      "Epoch 36/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5345 - accuracy: 0.7381\n",
      "Epoch 37/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5345 - accuracy: 0.7385\n",
      "Epoch 38/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5342 - accuracy: 0.7386\n",
      "Epoch 39/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5343 - accuracy: 0.7380\n",
      "Epoch 40/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5341 - accuracy: 0.7380\n",
      "Epoch 41/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5341 - accuracy: 0.7382\n",
      "Epoch 42/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5341 - accuracy: 0.7383\n",
      "Epoch 43/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5340 - accuracy: 0.7384\n",
      "Epoch 44/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5341 - accuracy: 0.7379\n",
      "Epoch 45/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5340 - accuracy: 0.7382\n",
      "Epoch 46/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5339 - accuracy: 0.7379\n",
      "Epoch 47/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5337 - accuracy: 0.7382\n",
      "Epoch 48/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5337 - accuracy: 0.7389\n",
      "Epoch 49/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5338 - accuracy: 0.7390\n",
      "Epoch 50/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5336 - accuracy: 0.7386\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad, total= 7.4min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 563901 samples\n",
      "Epoch 1/50\n",
      "563901/563901 [==============================] - 11s 20us/sample - loss: 0.5540 - accuracy: 0.7241\n",
      "Epoch 2/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5420 - accuracy: 0.7349\n",
      "Epoch 3/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5407 - accuracy: 0.7356\n",
      "Epoch 4/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5396 - accuracy: 0.7356\n",
      "Epoch 5/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5390 - accuracy: 0.7366\n",
      "Epoch 6/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5387 - accuracy: 0.7362\n",
      "Epoch 7/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5383 - accuracy: 0.7370\n",
      "Epoch 8/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5381 - accuracy: 0.7363\n",
      "Epoch 9/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5376 - accuracy: 0.7366\n",
      "Epoch 10/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5373 - accuracy: 0.7367\n",
      "Epoch 11/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5371 - accuracy: 0.7378\n",
      "Epoch 12/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5369 - accuracy: 0.7366\n",
      "Epoch 13/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5368 - accuracy: 0.7368\n",
      "Epoch 14/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5364 - accuracy: 0.7378\n",
      "Epoch 15/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5364 - accuracy: 0.7381\n",
      "Epoch 16/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5360 - accuracy: 0.7378\n",
      "Epoch 17/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5363 - accuracy: 0.7374\n",
      "Epoch 18/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5358 - accuracy: 0.7376\n",
      "Epoch 19/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5358 - accuracy: 0.7377\n",
      "Epoch 20/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5359 - accuracy: 0.7377\n",
      "Epoch 21/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5356 - accuracy: 0.7379\n",
      "Epoch 22/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5358 - accuracy: 0.7378\n",
      "Epoch 23/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5352 - accuracy: 0.7384\n",
      "Epoch 24/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5352 - accuracy: 0.7375\n",
      "Epoch 25/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5352 - accuracy: 0.7384\n",
      "Epoch 26/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5351 - accuracy: 0.7384\n",
      "Epoch 27/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5351 - accuracy: 0.7375\n",
      "Epoch 28/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5350 - accuracy: 0.7384\n",
      "Epoch 29/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5348 - accuracy: 0.7383\n",
      "Epoch 30/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5348 - accuracy: 0.7384\n",
      "Epoch 31/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5346 - accuracy: 0.7387\n",
      "Epoch 32/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5348 - accuracy: 0.7382\n",
      "Epoch 33/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5346 - accuracy: 0.7388\n",
      "Epoch 34/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5347 - accuracy: 0.7392\n",
      "Epoch 35/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5346 - accuracy: 0.7378\n",
      "Epoch 36/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5342 - accuracy: 0.7383\n",
      "Epoch 37/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5345 - accuracy: 0.7390\n",
      "Epoch 38/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5344 - accuracy: 0.7384\n",
      "Epoch 39/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5345 - accuracy: 0.7385\n",
      "Epoch 40/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5341 - accuracy: 0.7387\n",
      "Epoch 41/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5341 - accuracy: 0.7384\n",
      "Epoch 42/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5340 - accuracy: 0.7386\n",
      "Epoch 43/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5340 - accuracy: 0.7384\n",
      "Epoch 44/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5342 - accuracy: 0.7390\n",
      "Epoch 45/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5340 - accuracy: 0.7387\n",
      "Epoch 46/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5341 - accuracy: 0.7387\n",
      "Epoch 47/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5339 - accuracy: 0.7390\n",
      "Epoch 48/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5337 - accuracy: 0.7392\n",
      "Epoch 49/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5339 - accuracy: 0.7388\n",
      "Epoch 50/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5336 - accuracy: 0.7387\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad, total= 7.1min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad \n",
      "Train on 563901 samples\n",
      "Epoch 1/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5550 - accuracy: 0.7250\n",
      "Epoch 2/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5419 - accuracy: 0.7344\n",
      "Epoch 3/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5406 - accuracy: 0.7362\n",
      "Epoch 4/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5398 - accuracy: 0.7374\n",
      "Epoch 5/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5395 - accuracy: 0.7370\n",
      "Epoch 6/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5387 - accuracy: 0.7378\n",
      "Epoch 7/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5384 - accuracy: 0.7382\n",
      "Epoch 8/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5378 - accuracy: 0.7377\n",
      "Epoch 9/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5378 - accuracy: 0.7376\n",
      "Epoch 10/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5373 - accuracy: 0.7379\n",
      "Epoch 11/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5372 - accuracy: 0.7383\n",
      "Epoch 12/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5370 - accuracy: 0.7377\n",
      "Epoch 13/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5367 - accuracy: 0.7381\n",
      "Epoch 14/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5364 - accuracy: 0.7378\n",
      "Epoch 15/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5361 - accuracy: 0.7375\n",
      "Epoch 16/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5361 - accuracy: 0.7388\n",
      "Epoch 17/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5363 - accuracy: 0.7381\n",
      "Epoch 18/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5358 - accuracy: 0.7378\n",
      "Epoch 19/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5358 - accuracy: 0.7390\n",
      "Epoch 20/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5357 - accuracy: 0.7380\n",
      "Epoch 21/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5355 - accuracy: 0.7387\n",
      "Epoch 22/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5356 - accuracy: 0.7390\n",
      "Epoch 23/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5355 - accuracy: 0.7388\n",
      "Epoch 24/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5356 - accuracy: 0.7390\n",
      "Epoch 25/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5350 - accuracy: 0.7390\n",
      "Epoch 26/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5352 - accuracy: 0.7384\n",
      "Epoch 27/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5351 - accuracy: 0.7386\n",
      "Epoch 28/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5349 - accuracy: 0.7392\n",
      "Epoch 29/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5348 - accuracy: 0.7393\n",
      "Epoch 30/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5347 - accuracy: 0.7389\n",
      "Epoch 31/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5348 - accuracy: 0.7391\n",
      "Epoch 32/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5347 - accuracy: 0.7387\n",
      "Epoch 33/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5345 - accuracy: 0.7395\n",
      "Epoch 34/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5345 - accuracy: 0.7393\n",
      "Epoch 35/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5342 - accuracy: 0.7394\n",
      "Epoch 36/50\n",
      "563901/563901 [==============================] - 8s 14us/sample - loss: 0.5342 - accuracy: 0.7395\n",
      "Epoch 37/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5345 - accuracy: 0.7393\n",
      "Epoch 38/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5344 - accuracy: 0.7391\n",
      "Epoch 39/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5341 - accuracy: 0.7390\n",
      "Epoch 40/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5344 - accuracy: 0.7393\n",
      "Epoch 41/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5345 - accuracy: 0.7394\n",
      "Epoch 42/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5343 - accuracy: 0.7395\n",
      "Epoch 43/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5341 - accuracy: 0.7389\n",
      "Epoch 44/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5339 - accuracy: 0.7394\n",
      "Epoch 45/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5338 - accuracy: 0.7389\n",
      "Epoch 46/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5340 - accuracy: 0.7390\n",
      "Epoch 47/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5340 - accuracy: 0.7396\n",
      "Epoch 48/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5339 - accuracy: 0.7391\n",
      "Epoch 49/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5338 - accuracy: 0.7400\n",
      "Epoch 50/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5336 - accuracy: 0.7398\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad, total= 7.1min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad \n",
      "Train on 563901 samples\n",
      "Epoch 1/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5559 - accuracy: 0.7229\n",
      "Epoch 2/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5433 - accuracy: 0.7336\n",
      "Epoch 3/50\n",
      "563901/563901 [==============================] - 8s 14us/sample - loss: 0.5416 - accuracy: 0.7337\n",
      "Epoch 4/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5409 - accuracy: 0.7351\n",
      "Epoch 5/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5401 - accuracy: 0.7354\n",
      "Epoch 6/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5397 - accuracy: 0.7358\n",
      "Epoch 7/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5395 - accuracy: 0.7357\n",
      "Epoch 8/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5389 - accuracy: 0.7349\n",
      "Epoch 9/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5388 - accuracy: 0.7352\n",
      "Epoch 10/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5384 - accuracy: 0.7362\n",
      "Epoch 11/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5381 - accuracy: 0.7362\n",
      "Epoch 12/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5382 - accuracy: 0.7363\n",
      "Epoch 13/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5378 - accuracy: 0.7366\n",
      "Epoch 14/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5373 - accuracy: 0.7364\n",
      "Epoch 15/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5374 - accuracy: 0.7365\n",
      "Epoch 16/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5372 - accuracy: 0.7375\n",
      "Epoch 17/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5371 - accuracy: 0.7372\n",
      "Epoch 18/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5372 - accuracy: 0.7374\n",
      "Epoch 19/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5369 - accuracy: 0.7370\n",
      "Epoch 20/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5369 - accuracy: 0.7373\n",
      "Epoch 21/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5368 - accuracy: 0.7369\n",
      "Epoch 22/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5367 - accuracy: 0.7377\n",
      "Epoch 23/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5366 - accuracy: 0.7371\n",
      "Epoch 24/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5364 - accuracy: 0.7373\n",
      "Epoch 25/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5363 - accuracy: 0.7379\n",
      "Epoch 26/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5362 - accuracy: 0.7377\n",
      "Epoch 27/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5362 - accuracy: 0.7369\n",
      "Epoch 28/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5361 - accuracy: 0.7379\n",
      "Epoch 29/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5359 - accuracy: 0.7375\n",
      "Epoch 30/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5359 - accuracy: 0.7376\n",
      "Epoch 31/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5357 - accuracy: 0.7377\n",
      "Epoch 32/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5358 - accuracy: 0.7376\n",
      "Epoch 33/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5359 - accuracy: 0.7375\n",
      "Epoch 34/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5356 - accuracy: 0.7379\n",
      "Epoch 35/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5357 - accuracy: 0.7373\n",
      "Epoch 36/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5357 - accuracy: 0.7372\n",
      "Epoch 37/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5357 - accuracy: 0.7377\n",
      "Epoch 38/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5355 - accuracy: 0.7377\n",
      "Epoch 39/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5356 - accuracy: 0.7374\n",
      "Epoch 40/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5353 - accuracy: 0.7378\n",
      "Epoch 41/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5352 - accuracy: 0.7378\n",
      "Epoch 42/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5353 - accuracy: 0.7380\n",
      "Epoch 43/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5354 - accuracy: 0.7382\n",
      "Epoch 44/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5350 - accuracy: 0.7384\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5354 - accuracy: 0.7376\n",
      "Epoch 46/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5351 - accuracy: 0.7379\n",
      "Epoch 47/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5351 - accuracy: 0.7388\n",
      "Epoch 48/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5350 - accuracy: 0.7377\n",
      "Epoch 49/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5352 - accuracy: 0.7384\n",
      "Epoch 50/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5347 - accuracy: 0.7382\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad, total= 7.1min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad \n",
      "Train on 563901 samples\n",
      "Epoch 1/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5546 - accuracy: 0.7239\n",
      "Epoch 2/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5419 - accuracy: 0.7350\n",
      "Epoch 3/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5409 - accuracy: 0.7359\n",
      "Epoch 4/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5398 - accuracy: 0.7374\n",
      "Epoch 5/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5392 - accuracy: 0.7367\n",
      "Epoch 6/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5387 - accuracy: 0.7378\n",
      "Epoch 7/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5380 - accuracy: 0.7375\n",
      "Epoch 8/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5375 - accuracy: 0.7386\n",
      "Epoch 9/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5376 - accuracy: 0.7377\n",
      "Epoch 10/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5372 - accuracy: 0.7381\n",
      "Epoch 11/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5369 - accuracy: 0.7380\n",
      "Epoch 12/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5366 - accuracy: 0.7377\n",
      "Epoch 13/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5366 - accuracy: 0.7374\n",
      "Epoch 14/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5363 - accuracy: 0.7375\n",
      "Epoch 15/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5364 - accuracy: 0.7377\n",
      "Epoch 16/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5363 - accuracy: 0.7385\n",
      "Epoch 17/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5359 - accuracy: 0.7376\n",
      "Epoch 18/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5359 - accuracy: 0.7381\n",
      "Epoch 19/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5360 - accuracy: 0.7380\n",
      "Epoch 20/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5358 - accuracy: 0.7378\n",
      "Epoch 21/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5358 - accuracy: 0.7382\n",
      "Epoch 22/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5356 - accuracy: 0.7380\n",
      "Epoch 23/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5354 - accuracy: 0.7385\n",
      "Epoch 24/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5349 - accuracy: 0.7386\n",
      "Epoch 25/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5354 - accuracy: 0.7377\n",
      "Epoch 26/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5351 - accuracy: 0.7374\n",
      "Epoch 27/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5350 - accuracy: 0.7382\n",
      "Epoch 28/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5351 - accuracy: 0.7381\n",
      "Epoch 29/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5348 - accuracy: 0.7378\n",
      "Epoch 30/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5350 - accuracy: 0.7391\n",
      "Epoch 31/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5348 - accuracy: 0.7379\n",
      "Epoch 32/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5348 - accuracy: 0.7384\n",
      "Epoch 33/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5348 - accuracy: 0.7386\n",
      "Epoch 34/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5347 - accuracy: 0.7388\n",
      "Epoch 35/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5347 - accuracy: 0.7383\n",
      "Epoch 36/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5345 - accuracy: 0.7392\n",
      "Epoch 37/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5345 - accuracy: 0.7389\n",
      "Epoch 38/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5344 - accuracy: 0.7380\n",
      "Epoch 39/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5342 - accuracy: 0.7389\n",
      "Epoch 40/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5341 - accuracy: 0.7387\n",
      "Epoch 41/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5342 - accuracy: 0.7387\n",
      "Epoch 42/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5345 - accuracy: 0.7379\n",
      "Epoch 43/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5340 - accuracy: 0.7382\n",
      "Epoch 44/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5344 - accuracy: 0.7386\n",
      "Epoch 45/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5340 - accuracy: 0.7384\n",
      "Epoch 46/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5341 - accuracy: 0.7385\n",
      "Epoch 47/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5339 - accuracy: 0.7385\n",
      "Epoch 48/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5339 - accuracy: 0.7390\n",
      "Epoch 49/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5336 - accuracy: 0.7395\n",
      "Epoch 50/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5339 - accuracy: 0.7383\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adagrad, total= 7.2min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd \n",
      "Train on 563900 samples\n",
      "Epoch 1/50\n",
      "563900/563900 [==============================] - 10s 18us/sample - loss: 0.5539 - accuracy: 0.7247\n",
      "Epoch 2/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5418 - accuracy: 0.7343\n",
      "Epoch 3/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5403 - accuracy: 0.7363\n",
      "Epoch 4/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5395 - accuracy: 0.7361\n",
      "Epoch 5/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5388 - accuracy: 0.7357\n",
      "Epoch 6/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5387 - accuracy: 0.7369\n",
      "Epoch 7/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5383 - accuracy: 0.7363\n",
      "Epoch 8/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5376 - accuracy: 0.7371\n",
      "Epoch 9/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5371 - accuracy: 0.7373\n",
      "Epoch 10/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5375 - accuracy: 0.7367\n",
      "Epoch 11/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5368 - accuracy: 0.7364\n",
      "Epoch 12/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5367 - accuracy: 0.7374\n",
      "Epoch 13/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5366 - accuracy: 0.7372\n",
      "Epoch 14/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5361 - accuracy: 0.7372\n",
      "Epoch 15/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5361 - accuracy: 0.7381\n",
      "Epoch 16/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5359 - accuracy: 0.7379\n",
      "Epoch 17/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5358 - accuracy: 0.7369\n",
      "Epoch 18/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5356 - accuracy: 0.7378\n",
      "Epoch 19/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5358 - accuracy: 0.7379\n",
      "Epoch 20/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5355 - accuracy: 0.7374\n",
      "Epoch 21/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5355 - accuracy: 0.7385\n",
      "Epoch 22/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5352 - accuracy: 0.7380\n",
      "Epoch 23/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5353 - accuracy: 0.7377\n",
      "Epoch 24/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5354 - accuracy: 0.7371\n",
      "Epoch 25/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5351 - accuracy: 0.7380\n",
      "Epoch 26/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5351 - accuracy: 0.7378\n",
      "Epoch 27/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5348 - accuracy: 0.7380\n",
      "Epoch 28/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5349 - accuracy: 0.7381\n",
      "Epoch 29/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5348 - accuracy: 0.7385\n",
      "Epoch 30/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5347 - accuracy: 0.7380\n",
      "Epoch 31/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5349 - accuracy: 0.7378\n",
      "Epoch 32/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5347 - accuracy: 0.7380\n",
      "Epoch 33/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5345 - accuracy: 0.7385\n",
      "Epoch 34/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5343 - accuracy: 0.7382\n",
      "Epoch 35/50\n",
      "563900/563900 [==============================] - 8s 14us/sample - loss: 0.5344 - accuracy: 0.7381\n",
      "Epoch 36/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5344 - accuracy: 0.7381\n",
      "Epoch 37/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5342 - accuracy: 0.7383\n",
      "Epoch 38/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5340 - accuracy: 0.7380\n",
      "Epoch 39/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5341 - accuracy: 0.7383\n",
      "Epoch 40/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5342 - accuracy: 0.7385\n",
      "Epoch 41/50\n",
      "563900/563900 [==============================] - 9s 15us/sample - loss: 0.5343 - accuracy: 0.7380\n",
      "Epoch 42/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5344 - accuracy: 0.7382\n",
      "Epoch 43/50\n",
      "563900/563900 [==============================] - 9s 17us/sample - loss: 0.5342 - accuracy: 0.7383\n",
      "Epoch 44/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5342 - accuracy: 0.7388\n",
      "Epoch 45/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5338 - accuracy: 0.7386\n",
      "Epoch 46/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5338 - accuracy: 0.7384\n",
      "Epoch 47/50\n",
      "563900/563900 [==============================] - 10s 17us/sample - loss: 0.5338 - accuracy: 0.7387\n",
      "Epoch 48/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5337 - accuracy: 0.7387\n",
      "Epoch 49/50\n",
      "563900/563900 [==============================] - 10s 18us/sample - loss: 0.5338 - accuracy: 0.7386\n",
      "Epoch 50/50\n",
      "563900/563900 [==============================] - 10s 18us/sample - loss: 0.5335 - accuracy: 0.7390\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd, total= 7.3min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd \n",
      "Train on 563901 samples\n",
      "Epoch 1/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5547 - accuracy: 0.7235\n",
      "Epoch 2/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5421 - accuracy: 0.7334\n",
      "Epoch 3/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5407 - accuracy: 0.7357\n",
      "Epoch 4/50\n",
      "563901/563901 [==============================] - 8s 14us/sample - loss: 0.5398 - accuracy: 0.7369\n",
      "Epoch 5/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5393 - accuracy: 0.7365\n",
      "Epoch 6/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5387 - accuracy: 0.7367\n",
      "Epoch 7/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5383 - accuracy: 0.7373\n",
      "Epoch 8/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5379 - accuracy: 0.7369\n",
      "Epoch 9/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5375 - accuracy: 0.7372\n",
      "Epoch 10/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5376 - accuracy: 0.7372\n",
      "Epoch 11/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5374 - accuracy: 0.7371\n",
      "Epoch 12/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5369 - accuracy: 0.7373\n",
      "Epoch 13/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5366 - accuracy: 0.7370\n",
      "Epoch 14/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5365 - accuracy: 0.7372\n",
      "Epoch 15/50\n",
      "563901/563901 [==============================] - 9s 17us/sample - loss: 0.5363 - accuracy: 0.7374\n",
      "Epoch 16/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5361 - accuracy: 0.7376\n",
      "Epoch 17/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5360 - accuracy: 0.7376\n",
      "Epoch 18/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5360 - accuracy: 0.7377\n",
      "Epoch 19/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5358 - accuracy: 0.7380\n",
      "Epoch 20/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5358 - accuracy: 0.7378\n",
      "Epoch 21/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5354 - accuracy: 0.7374\n",
      "Epoch 22/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5355 - accuracy: 0.7378\n",
      "Epoch 23/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5352 - accuracy: 0.7378\n",
      "Epoch 24/50\n",
      "563901/563901 [==============================] - 11s 20us/sample - loss: 0.5352 - accuracy: 0.7376\n",
      "Epoch 25/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5353 - accuracy: 0.7387\n",
      "Epoch 26/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5349 - accuracy: 0.7371\n",
      "Epoch 27/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5351 - accuracy: 0.7382\n",
      "Epoch 28/50\n",
      "563901/563901 [==============================] - 17s 29us/sample - loss: 0.5350 - accuracy: 0.7384\n",
      "Epoch 29/50\n",
      "563901/563901 [==============================] - 12s 22us/sample - loss: 0.5347 - accuracy: 0.7385\n",
      "Epoch 30/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5348 - accuracy: 0.7376\n",
      "Epoch 31/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5349 - accuracy: 0.7385\n",
      "Epoch 32/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5350 - accuracy: 0.7387\n",
      "Epoch 33/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5345 - accuracy: 0.7394\n",
      "Epoch 34/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5346 - accuracy: 0.7389\n",
      "Epoch 35/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5344 - accuracy: 0.7387\n",
      "Epoch 36/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5344 - accuracy: 0.7387\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5341 - accuracy: 0.7391\n",
      "Epoch 38/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5344 - accuracy: 0.7391\n",
      "Epoch 39/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5343 - accuracy: 0.7388\n",
      "Epoch 40/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5345 - accuracy: 0.7391\n",
      "Epoch 41/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5342 - accuracy: 0.7386\n",
      "Epoch 42/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5342 - accuracy: 0.7389\n",
      "Epoch 43/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5342 - accuracy: 0.7387\n",
      "Epoch 44/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5339 - accuracy: 0.7386\n",
      "Epoch 45/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5339 - accuracy: 0.7389\n",
      "Epoch 46/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5342 - accuracy: 0.7387\n",
      "Epoch 47/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5337 - accuracy: 0.7397\n",
      "Epoch 48/50\n",
      "563901/563901 [==============================] - 9s 17us/sample - loss: 0.5338 - accuracy: 0.7392\n",
      "Epoch 49/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5338 - accuracy: 0.7390\n",
      "Epoch 50/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5338 - accuracy: 0.7393\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd, total= 7.8min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd \n",
      "Train on 563901 samples\n",
      "Epoch 1/50\n",
      "563901/563901 [==============================] - 10s 19us/sample - loss: 0.5548 - accuracy: 0.7237\n",
      "Epoch 2/50\n",
      "563901/563901 [==============================] - 15s 26us/sample - loss: 0.5423 - accuracy: 0.7346\n",
      "Epoch 3/50\n",
      "563901/563901 [==============================] - 19s 34us/sample - loss: 0.5405 - accuracy: 0.7360\n",
      "Epoch 4/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5397 - accuracy: 0.7366\n",
      "Epoch 5/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5392 - accuracy: 0.7365\n",
      "Epoch 6/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5386 - accuracy: 0.7375\n",
      "Epoch 7/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5382 - accuracy: 0.7370\n",
      "Epoch 8/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5379 - accuracy: 0.7380\n",
      "Epoch 9/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5376 - accuracy: 0.7371\n",
      "Epoch 10/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5372 - accuracy: 0.7378\n",
      "Epoch 11/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5372 - accuracy: 0.7382\n",
      "Epoch 12/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5369 - accuracy: 0.7379\n",
      "Epoch 13/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5366 - accuracy: 0.7390\n",
      "Epoch 14/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5364 - accuracy: 0.7381\n",
      "Epoch 15/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5363 - accuracy: 0.7382\n",
      "Epoch 16/50\n",
      "563901/563901 [==============================] - 9s 17us/sample - loss: 0.5360 - accuracy: 0.7382\n",
      "Epoch 17/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5363 - accuracy: 0.7385\n",
      "Epoch 18/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5359 - accuracy: 0.7381\n",
      "Epoch 19/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5360 - accuracy: 0.7378\n",
      "Epoch 20/50\n",
      "563901/563901 [==============================] - 13s 24us/sample - loss: 0.5356 - accuracy: 0.7384\n",
      "Epoch 21/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5359 - accuracy: 0.7384\n",
      "Epoch 22/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5354 - accuracy: 0.7385\n",
      "Epoch 23/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5355 - accuracy: 0.7384\n",
      "Epoch 24/50\n",
      "563901/563901 [==============================] - 13s 24us/sample - loss: 0.5355 - accuracy: 0.7385\n",
      "Epoch 25/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5353 - accuracy: 0.7382\n",
      "Epoch 26/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5352 - accuracy: 0.7397\n",
      "Epoch 27/50\n",
      "563901/563901 [==============================] - 17s 30us/sample - loss: 0.5348 - accuracy: 0.7395\n",
      "Epoch 28/50\n",
      "563901/563901 [==============================] - 15s 27us/sample - loss: 0.5351 - accuracy: 0.7396\n",
      "Epoch 29/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5350 - accuracy: 0.7384\n",
      "Epoch 30/50\n",
      "563901/563901 [==============================] - 17s 29us/sample - loss: 0.5351 - accuracy: 0.7386\n",
      "Epoch 31/50\n",
      "563901/563901 [==============================] - 21s 38us/sample - loss: 0.5348 - accuracy: 0.7384\n",
      "Epoch 32/50\n",
      "563901/563901 [==============================] - 20s 36us/sample - loss: 0.5347 - accuracy: 0.7388\n",
      "Epoch 33/50\n",
      "563901/563901 [==============================] - 16s 29us/sample - loss: 0.5345 - accuracy: 0.7389\n",
      "Epoch 34/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5347 - accuracy: 0.7400\n",
      "Epoch 35/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5347 - accuracy: 0.7385\n",
      "Epoch 36/50\n",
      "563901/563901 [==============================] - 14s 26us/sample - loss: 0.5347 - accuracy: 0.7386\n",
      "Epoch 37/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5345 - accuracy: 0.7386\n",
      "Epoch 38/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5345 - accuracy: 0.7398\n",
      "Epoch 39/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5344 - accuracy: 0.7392\n",
      "Epoch 40/50\n",
      "563901/563901 [==============================] - 14s 24us/sample - loss: 0.5344 - accuracy: 0.7389\n",
      "Epoch 41/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5341 - accuracy: 0.7390\n",
      "Epoch 42/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5342 - accuracy: 0.7391\n",
      "Epoch 43/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5340 - accuracy: 0.7395\n",
      "Epoch 44/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5339 - accuracy: 0.7389\n",
      "Epoch 45/50\n",
      "563901/563901 [==============================] - 14s 26us/sample - loss: 0.5340 - accuracy: 0.7394\n",
      "Epoch 46/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5340 - accuracy: 0.7402\n",
      "Epoch 47/50\n",
      "563901/563901 [==============================] - 14s 25us/sample - loss: 0.5340 - accuracy: 0.7399\n",
      "Epoch 48/50\n",
      "563901/563901 [==============================] - 14s 26us/sample - loss: 0.5340 - accuracy: 0.7396\n",
      "Epoch 49/50\n",
      "563901/563901 [==============================] - 11s 20us/sample - loss: 0.5335 - accuracy: 0.7392\n",
      "Epoch 50/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5338 - accuracy: 0.7399\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd, total=10.3min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd \n",
      "Train on 563901 samples\n",
      "Epoch 1/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5573 - accuracy: 0.7225\n",
      "Epoch 2/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5432 - accuracy: 0.7316\n",
      "Epoch 3/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5417 - accuracy: 0.7339\n",
      "Epoch 4/50\n",
      "563901/563901 [==============================] - 8s 15us/sample - loss: 0.5406 - accuracy: 0.7350\n",
      "Epoch 5/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5401 - accuracy: 0.7352\n",
      "Epoch 6/50\n",
      "563901/563901 [==============================] - 9s 17us/sample - loss: 0.5397 - accuracy: 0.7359\n",
      "Epoch 7/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5393 - accuracy: 0.7358\n",
      "Epoch 8/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5394 - accuracy: 0.7358\n",
      "Epoch 9/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5386 - accuracy: 0.7362\n",
      "Epoch 10/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5385 - accuracy: 0.7370\n",
      "Epoch 11/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5381 - accuracy: 0.7359\n",
      "Epoch 12/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5377 - accuracy: 0.7374\n",
      "Epoch 13/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5378 - accuracy: 0.7365\n",
      "Epoch 14/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5375 - accuracy: 0.7370\n",
      "Epoch 15/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5372 - accuracy: 0.7371\n",
      "Epoch 16/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5372 - accuracy: 0.7369\n",
      "Epoch 17/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5372 - accuracy: 0.7375\n",
      "Epoch 18/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5369 - accuracy: 0.7370\n",
      "Epoch 19/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5365 - accuracy: 0.7367\n",
      "Epoch 20/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5365 - accuracy: 0.7377\n",
      "Epoch 21/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5367 - accuracy: 0.7374\n",
      "Epoch 22/50\n",
      "563901/563901 [==============================] - 13s 23us/sample - loss: 0.5366 - accuracy: 0.7363\n",
      "Epoch 23/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5363 - accuracy: 0.7373\n",
      "Epoch 24/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5364 - accuracy: 0.7367\n",
      "Epoch 25/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5360 - accuracy: 0.7372\n",
      "Epoch 26/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5362 - accuracy: 0.7367\n",
      "Epoch 27/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5361 - accuracy: 0.7369\n",
      "Epoch 28/50\n",
      "563901/563901 [==============================] - 13s 22us/sample - loss: 0.5363 - accuracy: 0.7373\n",
      "Epoch 29/50\n",
      "563901/563901 [==============================] - 13s 23us/sample - loss: 0.5359 - accuracy: 0.7378\n",
      "Epoch 30/50\n",
      "563901/563901 [==============================] - 13s 24us/sample - loss: 0.5361 - accuracy: 0.7371\n",
      "Epoch 31/50\n",
      "563901/563901 [==============================] - 13s 24us/sample - loss: 0.5355 - accuracy: 0.7374\n",
      "Epoch 32/50\n",
      "563901/563901 [==============================] - 13s 23us/sample - loss: 0.5360 - accuracy: 0.7373\n",
      "Epoch 33/50\n",
      "563901/563901 [==============================] - 13s 23us/sample - loss: 0.5358 - accuracy: 0.7375\n",
      "Epoch 34/50\n",
      "563901/563901 [==============================] - 12s 22us/sample - loss: 0.5356 - accuracy: 0.7373\n",
      "Epoch 35/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5354 - accuracy: 0.7376\n",
      "Epoch 36/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5353 - accuracy: 0.7379\n",
      "Epoch 37/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5354 - accuracy: 0.7373\n",
      "Epoch 38/50\n",
      "563901/563901 [==============================] - 11s 20us/sample - loss: 0.5356 - accuracy: 0.7374\n",
      "Epoch 39/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5352 - accuracy: 0.7375\n",
      "Epoch 40/50\n",
      "563901/563901 [==============================] - 11s 20us/sample - loss: 0.5354 - accuracy: 0.7377\n",
      "Epoch 41/50\n",
      "563901/563901 [==============================] - 12s 21us/sample - loss: 0.5353 - accuracy: 0.7368\n",
      "Epoch 42/50\n",
      "563901/563901 [==============================] - 11s 20us/sample - loss: 0.5352 - accuracy: 0.7373\n",
      "Epoch 43/50\n",
      "563901/563901 [==============================] - 11s 20us/sample - loss: 0.5352 - accuracy: 0.7379\n",
      "Epoch 44/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5351 - accuracy: 0.7383\n",
      "Epoch 45/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5350 - accuracy: 0.7375\n",
      "Epoch 46/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5349 - accuracy: 0.7374\n",
      "Epoch 47/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5348 - accuracy: 0.7380\n",
      "Epoch 48/50\n",
      "563901/563901 [==============================] - 9s 17us/sample - loss: 0.5351 - accuracy: 0.7376\n",
      "Epoch 49/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5351 - accuracy: 0.7375\n",
      "Epoch 50/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5345 - accuracy: 0.7383\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd, total= 9.0min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd \n",
      "Train on 563901 samples\n",
      "Epoch 1/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5559 - accuracy: 0.7234\n",
      "Epoch 2/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5420 - accuracy: 0.7341\n",
      "Epoch 3/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5409 - accuracy: 0.7362\n",
      "Epoch 4/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5398 - accuracy: 0.7367\n",
      "Epoch 5/50\n",
      "563901/563901 [==============================] - 9s 17us/sample - loss: 0.5392 - accuracy: 0.7367\n",
      "Epoch 6/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5386 - accuracy: 0.7374\n",
      "Epoch 7/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5384 - accuracy: 0.7371\n",
      "Epoch 8/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5378 - accuracy: 0.7378\n",
      "Epoch 9/50\n",
      "563901/563901 [==============================] - 11s 20us/sample - loss: 0.5378 - accuracy: 0.7381\n",
      "Epoch 10/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5372 - accuracy: 0.7368\n",
      "Epoch 11/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5371 - accuracy: 0.7373\n",
      "Epoch 12/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5368 - accuracy: 0.7371\n",
      "Epoch 13/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5366 - accuracy: 0.7378\n",
      "Epoch 14/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5363 - accuracy: 0.7377\n",
      "Epoch 15/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5362 - accuracy: 0.7379\n",
      "Epoch 16/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5364 - accuracy: 0.7379\n",
      "Epoch 17/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5362 - accuracy: 0.7380\n",
      "Epoch 18/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5360 - accuracy: 0.7379\n",
      "Epoch 19/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5358 - accuracy: 0.7373\n",
      "Epoch 20/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5358 - accuracy: 0.7379\n",
      "Epoch 21/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5356 - accuracy: 0.7385\n",
      "Epoch 22/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5356 - accuracy: 0.7380\n",
      "Epoch 23/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5357 - accuracy: 0.7380\n",
      "Epoch 24/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5352 - accuracy: 0.7380\n",
      "Epoch 25/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5352 - accuracy: 0.7382\n",
      "Epoch 26/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5352 - accuracy: 0.7380\n",
      "Epoch 27/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5354 - accuracy: 0.7382\n",
      "Epoch 28/50\n",
      "563901/563901 [==============================] - 11s 20us/sample - loss: 0.5347 - accuracy: 0.7383\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5353 - accuracy: 0.7384\n",
      "Epoch 30/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5349 - accuracy: 0.7382\n",
      "Epoch 31/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5349 - accuracy: 0.7388\n",
      "Epoch 32/50\n",
      "563901/563901 [==============================] - 9s 16us/sample - loss: 0.5348 - accuracy: 0.7386\n",
      "Epoch 33/50\n",
      "563901/563901 [==============================] - 9s 17us/sample - loss: 0.5347 - accuracy: 0.7385\n",
      "Epoch 34/50\n",
      "563901/563901 [==============================] - 9s 15us/sample - loss: 0.5347 - accuracy: 0.7386\n",
      "Epoch 35/50\n",
      "563901/563901 [==============================] - 9s 17us/sample - loss: 0.5346 - accuracy: 0.7379\n",
      "Epoch 36/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5346 - accuracy: 0.7385\n",
      "Epoch 37/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5346 - accuracy: 0.7387\n",
      "Epoch 38/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5345 - accuracy: 0.7384\n",
      "Epoch 39/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5346 - accuracy: 0.7386\n",
      "Epoch 40/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5345 - accuracy: 0.7391\n",
      "Epoch 41/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5345 - accuracy: 0.7382\n",
      "Epoch 42/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5340 - accuracy: 0.7387\n",
      "Epoch 43/50\n",
      "563901/563901 [==============================] - 11s 19us/sample - loss: 0.5344 - accuracy: 0.7389\n",
      "Epoch 44/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5342 - accuracy: 0.7388\n",
      "Epoch 45/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5343 - accuracy: 0.7389\n",
      "Epoch 46/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5341 - accuracy: 0.7389\n",
      "Epoch 47/50\n",
      "563901/563901 [==============================] - 10s 19us/sample - loss: 0.5343 - accuracy: 0.7395\n",
      "Epoch 48/50\n",
      "563901/563901 [==============================] - 9s 17us/sample - loss: 0.5342 - accuracy: 0.7390\n",
      "Epoch 49/50\n",
      "563901/563901 [==============================] - 10s 18us/sample - loss: 0.5340 - accuracy: 0.7391\n",
      "Epoch 50/50\n",
      "563901/563901 [==============================] - 10s 17us/sample - loss: 0.5340 - accuracy: 0.7391\n",
      "[CV]  activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=sgd, total= 8.3min\n",
      "[CV] activation=relu, batch_size=256, dropout_rate=0.2, learning_rate=0.001, optimizer=adam \n",
      "Train on 563900 samples\n",
      "Epoch 1/50\n",
      "563900/563900 [==============================] - 12s 21us/sample - loss: 0.5538 - accuracy: 0.7237\n",
      "Epoch 2/50\n",
      "563900/563900 [==============================] - 11s 20us/sample - loss: 0.5421 - accuracy: 0.7335\n",
      "Epoch 3/50\n",
      "563900/563900 [==============================] - 11s 19us/sample - loss: 0.5404 - accuracy: 0.7354\n",
      "Epoch 4/50\n",
      "563900/563900 [==============================] - 11s 20us/sample - loss: 0.5399 - accuracy: 0.7353\n",
      "Epoch 5/50\n",
      "563900/563900 [==============================] - 11s 20us/sample - loss: 0.5392 - accuracy: 0.7371\n",
      "Epoch 6/50\n",
      "563900/563900 [==============================] - 11s 19us/sample - loss: 0.5385 - accuracy: 0.7359\n",
      "Epoch 7/50\n",
      "563900/563900 [==============================] - 10s 17us/sample - loss: 0.5382 - accuracy: 0.7364\n",
      "Epoch 8/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5378 - accuracy: 0.7363\n",
      "Epoch 9/50\n",
      "563900/563900 [==============================] - 10s 17us/sample - loss: 0.5374 - accuracy: 0.7368\n",
      "Epoch 10/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5374 - accuracy: 0.7363\n",
      "Epoch 11/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5370 - accuracy: 0.7368\n",
      "Epoch 12/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5368 - accuracy: 0.7369\n",
      "Epoch 13/50\n",
      "563900/563900 [==============================] - 9s 16us/sample - loss: 0.5364 - accuracy: 0.7365\n",
      "Epoch 14/50\n",
      "563900/563900 [==============================] - 8s 15us/sample - loss: 0.5362 - accuracy: 0.7365\n",
      "Epoch 15/50\n",
      "563900/563900 [==============================] - 10s 17us/sample - loss: 0.5363 - accuracy: 0.7369\n",
      "Epoch 16/50\n",
      "563900/563900 [==============================] - 10s 18us/sample - loss: 0.5361 - accuracy: 0.7372\n",
      "Epoch 17/50\n",
      "563900/563900 [==============================] - 11s 20us/sample - loss: 0.5360 - accuracy: 0.7370\n",
      "Epoch 18/50\n",
      "563900/563900 [==============================] - 11s 20us/sample - loss: 0.5358 - accuracy: 0.7369\n",
      "Epoch 19/50\n",
      "563900/563900 [==============================] - 11s 20us/sample - loss: 0.5354 - accuracy: 0.7369\n",
      "Epoch 20/50\n",
      "563900/563900 [==============================] - 10s 18us/sample - loss: 0.5356 - accuracy: 0.7372\n",
      "Epoch 21/50\n",
      "563900/563900 [==============================] - 11s 20us/sample - loss: 0.5354 - accuracy: 0.7371\n",
      "Epoch 22/50\n",
      "563900/563900 [==============================] - 11s 19us/sample - loss: 0.5354 - accuracy: 0.7369\n",
      "Epoch 23/50\n",
      "563900/563900 [==============================] - 12s 21us/sample - loss: 0.5354 - accuracy: 0.7370\n",
      "Epoch 24/50\n",
      "563900/563900 [==============================] - 10s 18us/sample - loss: 0.5351 - accuracy: 0.7379\n",
      "Epoch 25/50\n",
      "563900/563900 [==============================] - 11s 20us/sample - loss: 0.5354 - accuracy: 0.7366\n",
      "Epoch 26/50\n",
      "563900/563900 [==============================] - 12s 21us/sample - loss: 0.5348 - accuracy: 0.7376\n",
      "Epoch 27/50\n",
      "130816/563900 [=====>........................] - ETA: 8s - loss: 0.5370 - accuracy: 0.7372"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.2, optimizer='adam', activation='relu'):\n",
    "    model = Sequential([\n",
    "        BatchNormalization(input_shape=input_shape), \n",
    "        Dense(units=256, activation=activation),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate=dropout_rate), \n",
    "        Dense(units=256, activation=activation),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate=dropout_rate),\n",
    "        Dense(units=1, activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'batch_size': [256],\n",
    "    'dropout_rate': [0.2],\n",
    "    'optimizer': ['adagrad', 'sgd', 'adam', 'rmsprop'],\n",
    "    'learning_rate': [0.001, 0.005, 0.1],\n",
    "    'activation': ['relu', 'sigmoid', 'tanh']\n",
    "}\n",
    "\n",
    "# Define a wrapper function for the MLP NN model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=50)\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=2\n",
    ")    \n",
    "\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weight = {0: class_weights[0], 1: class_weights[1]} \n",
    "\n",
    "# Fit the GridSearchCV object\n",
    "grid_search.fit(X_train, y_train, class_weight=class_weight)\n",
    "\n",
    "# Evaluate the performance of the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = create_model(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    optimizer=best_params['optimizer'],\n",
    "    activation=best_params['activation']\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=best_params['batch_size'],\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
    "history_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n",
    "\n",
    "print((\"Best Validation Loss: {:0.4f}\" +\\\n",
    "      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
    "      .format(history_df['val_loss'].min(), \n",
    "              history_df['val_accuracy'].max()))\n",
    "\n",
    "# Generate some accuracy metrics\n",
    "score = best_model.evaluate(X_valid, y_valid)\n",
    "print('Validation accuracy:', score[1])\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 704876 samples, validate on 176220 samples\n",
      "Epoch 1/200\n",
      "704876/704876 [==============================] - 17s 24us/sample - loss: 0.5743 - accuracy: 0.7137 - val_loss: 0.6527 - val_accuracy: 0.5422\n",
      "Epoch 2/200\n",
      "704876/704876 [==============================] - 15s 21us/sample - loss: 0.5749 - accuracy: 0.7107 - val_loss: 1.9864 - val_accuracy: 0.5889\n",
      "Epoch 3/200\n",
      "704876/704876 [==============================] - 14s 20us/sample - loss: 0.5726 - accuracy: 0.7133 - val_loss: 1.2660 - val_accuracy: 0.6220\n",
      "Epoch 4/200\n",
      "704876/704876 [==============================] - 14s 20us/sample - loss: 0.5695 - accuracy: 0.7153 - val_loss: 0.5570 - val_accuracy: 0.6945\n",
      "Epoch 5/200\n",
      "704876/704876 [==============================] - 13s 19us/sample - loss: 0.5703 - accuracy: 0.7165 - val_loss: 6.2273 - val_accuracy: 0.6146\n",
      "Epoch 6/200\n",
      "704876/704876 [==============================] - 14s 19us/sample - loss: 0.5685 - accuracy: 0.7181 - val_loss: 13.0587 - val_accuracy: 0.5950\n",
      "Epoch 7/200\n",
      "704876/704876 [==============================] - 14s 20us/sample - loss: 0.5685 - accuracy: 0.7168 - val_loss: 103.7158 - val_accuracy: 0.5890\n",
      "Epoch 8/200\n",
      "704876/704876 [==============================] - 14s 20us/sample - loss: 0.5758 - accuracy: 0.7165 - val_loss: 0.7991 - val_accuracy: 0.5255\n",
      "Epoch 9/200\n",
      "704876/704876 [==============================] - 14s 19us/sample - loss: 0.5656 - accuracy: 0.7208 - val_loss: 18.6423 - val_accuracy: 0.7530\n",
      "Epoch 10/200\n",
      "704876/704876 [==============================] - 15s 21us/sample - loss: 0.5670 - accuracy: 0.7185 - val_loss: 6.7790 - val_accuracy: 0.6432\n",
      "Epoch 11/200\n",
      "704876/704876 [==============================] - 14s 20us/sample - loss: 0.5661 - accuracy: 0.7203 - val_loss: 6.3556 - val_accuracy: 0.7615\n",
      "Epoch 12/200\n",
      "704876/704876 [==============================] - 14s 20us/sample - loss: 0.5658 - accuracy: 0.7193 - val_loss: 243.0413 - val_accuracy: 0.7756\n",
      "Epoch 13/200\n",
      "704876/704876 [==============================] - 14s 19us/sample - loss: 0.5634 - accuracy: 0.7217 - val_loss: 38.6351 - val_accuracy: 0.6239\n",
      "Epoch 14/200\n",
      "704876/704876 [==============================] - 13s 19us/sample - loss: 0.5644 - accuracy: 0.7207 - val_loss: 19.6672 - val_accuracy: 0.8202\n",
      "Epoch 15/200\n",
      "704876/704876 [==============================] - 14s 19us/sample - loss: 0.5651 - accuracy: 0.7200 - val_loss: 3.5091 - val_accuracy: 0.6583\n",
      "Epoch 16/200\n",
      "704876/704876 [==============================] - 13s 18us/sample - loss: 0.5621 - accuracy: 0.7221 - val_loss: 51.4824 - val_accuracy: 0.7679\n",
      "Epoch 17/200\n",
      "704876/704876 [==============================] - 12s 18us/sample - loss: 0.5628 - accuracy: 0.7219 - val_loss: 17.9594 - val_accuracy: 0.6906\n",
      "Epoch 18/200\n",
      "704876/704876 [==============================] - 12s 17us/sample - loss: 0.5669 - accuracy: 0.7198 - val_loss: 2.1842 - val_accuracy: 0.7591\n",
      "Epoch 19/200\n",
      "704876/704876 [==============================] - 14s 19us/sample - loss: 0.5704 - accuracy: 0.7174 - val_loss: 1242.4357 - val_accuracy: 0.8039\n",
      "Best Validation Loss: 0.5570\n",
      "Best Validation Accuracy: 0.8202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.69448984\n",
      "Best: 0.747612 using {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'rmsprop'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Evaluate the performance of the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = create_model(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    dropout_rate=best_params['dropout_rate'],\n",
    "    optimizer= best_params['optimizer'],\n",
    "    activation= best_params['activation']\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=15,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=best_params['batch_size'],\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=class_weight\n",
    ")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\n",
    "history_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n",
    "\n",
    "print((\"Best Validation Loss: {:0.4f}\" +\\\n",
    "      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
    "      .format(history_df['val_loss'].min(), \n",
    "              history_df['val_accuracy'].max()))\n",
    "\n",
    "# Generate some accuracy metrics\n",
    "score = best_model.evaluate(X_valid, y_valid)\n",
    "print('Validation accuracy:', score[1])\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "    #print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_183\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_549 (Bat (None, 11)                44        \n",
      "_________________________________________________________________\n",
      "dense_549 (Dense)            (None, 256)               3072      \n",
      "_________________________________________________________________\n",
      "batch_normalization_550 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_366 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_550 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_551 (Bat (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_367 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_551 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 71,213\n",
      "Trainable params: 70,167\n",
      "Non-trainable params: 1,046\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.69448984\n",
      "Best: 0.747612 using {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'rmsprop'}\n",
      "0.742634 (0.007699) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'adagrad'}\n",
      "0.736460 (0.008172) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n",
      "0.738784 (0.004064) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "0.738781 (0.012159) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'rmsprop'}\n",
      "0.743803 (0.009161) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'adagrad'}\n",
      "0.736090 (0.019709) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'sgd'}\n",
      "0.740391 (0.014057) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'adam'}\n",
      "0.742829 (0.012444) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'rmsprop'}\n",
      "0.740296 (0.041106) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'adagrad'}\n",
      "0.737345 (0.063215) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'sgd'}\n",
      "0.744555 (0.040582) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'adam'}\n",
      "0.747612 (0.052706) with: {'activation': 'relu', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'rmsprop'}\n",
      "0.733194 (0.004718) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'adagrad'}\n",
      "0.742091 (0.008765) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n",
      "0.735132 (0.006321) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "0.741989 (0.009567) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'rmsprop'}\n",
      "0.724140 (0.012326) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'adagrad'}\n",
      "0.735088 (0.011998) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'sgd'}\n",
      "0.737157 (0.026928) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'adam'}\n",
      "0.744555 (0.015282) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'rmsprop'}\n",
      "0.539351 (0.275614) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'adagrad'}\n",
      "0.647839 (0.228142) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'sgd'}\n",
      "0.588245 (0.216418) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'adam'}\n",
      "0.721259 (0.067712) with: {'activation': 'sigmoid', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'rmsprop'}\n",
      "0.747329 (0.002605) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'adagrad'}\n",
      "0.736307 (0.002742) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n",
      "0.745984 (0.009621) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "0.736711 (0.012806) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'optimizer': 'rmsprop'}\n",
      "0.735633 (0.017180) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'adagrad'}\n",
      "0.739615 (0.015623) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'sgd'}\n",
      "0.740861 (0.005624) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'adam'}\n",
      "0.734352 (0.017859) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.005, 'optimizer': 'rmsprop'}\n",
      "0.606865 (0.198135) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'adagrad'}\n",
      "0.678828 (0.238051) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'sgd'}\n",
      "0.625343 (0.216675) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'adam'}\n",
      "0.642533 (0.226506) with: {'activation': 'tanh', 'batch_size': 256, 'dropout_rate': 0.2, 'learning_rate': 0.1, 'optimizer': 'rmsprop'}\n"
     ]
    }
   ],
   "source": [
    "# Generate some accuracy metrics\n",
    "score = best_model.evaluate(X_valid, y_valid)\n",
    "print('Validation accuracy:', score[1])\n",
    "\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "params = grid_search.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmsprop\n",
      "0.2\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "print(best_params['optimizer'])\n",
    "print(best_params['dropout_rate'])\n",
    "print(best_params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the training data, confusion matrix and discriminator curve\n",
    "\n",
    "gpu_available = tf.test.is_gpu_available()\n",
    "print(gpu_available)   # check still on the gpu\n",
    "\n",
    "model = best_model\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_train, y_train, verbose = 2)    \n",
    "\n",
    "print(\"Validation loss:\", val_loss)\n",
    "print(\"Validation accuracy:\", val_acc)\n",
    "\n",
    "y_pred = model.predict(X_train, verbose = 2)\n",
    "y_pred_round = np.round(y_pred)\n",
    "#print(y_pred)\n",
    "\n",
    "# compute the confusion matrix\n",
    "\n",
    "cm = confusion_matrix(y_train, y_pred_round, normalize = 'true')   #, normalize = 'true')  #,'true' pred, all},)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Background', 'Signal']) # other env\n",
    "\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "print('Total signal processes;', cm[1,0]+cm[1,1])\n",
    "print('Total background processes;', cm[0,0]+cm[0,1])\n",
    "\n",
    "# Check if order of y_valid is the same order as y_pred\n",
    "# expect to be the ssame order if accuracy is same as validation accuracy above.\n",
    "\n",
    "# list of true outcomes\n",
    "y_train_list = list(y_train)\n",
    "\n",
    "# list of rounded predictions \n",
    "y_pred_round_list = list(y_pred_round)\n",
    "\n",
    "data_test = {'True_Val': y_train_list, 'Predicted': list(y_pred), 'Round Prediction': y_pred_round_list}\n",
    "df_test = pd.DataFrame(data_test)\n",
    "\n",
    "# Easiest to reorder df, so all 0 (background) are first and then 1's (signal)\n",
    "\n",
    "#Verified, works\n",
    "df_test = df_test.sort_values(by='True_Val')\n",
    "df_test\n",
    "\n",
    "# Convert arrays to scalars\n",
    "df_test['Round Prediction'] = df_test['Round Prediction'].apply(lambda x: x[0])\n",
    "        \n",
    "pred_one = df_test['Round Prediction'].value_counts()[1.0]\n",
    "ones_in_set = df_test['True_Val'].value_counts()[1]\n",
    "\n",
    "# Print the result\n",
    "print(pred_one)\n",
    "print('There is', ones_in_set, 'signal in the set')\n",
    "print('The model predicted',pred_one-ones_in_set ,'more ones than in reality')\n",
    "\n",
    "\n",
    "pred_zeros = df_test['Round Prediction'].value_counts()[0.0]\n",
    "zeros_in_set = df_test['True_Val'].value_counts()[0]\n",
    "\n",
    "# Print the result\n",
    "print(pred_zeros)\n",
    "print('There is', zeros_in_set, 'background in the set')   #agrees w/ confusion matrix\n",
    "print('The model predicted', pred_zeros-zeros_in_set,'more ones than in reality')\n",
    "\n",
    "\n",
    "#Split df into signal and bg\n",
    "\n",
    "df_bg = df_test[df_test['True_Val'] == 0]\n",
    "df_signal = df_test[df_test['True_Val'] == 1]\n",
    "\n",
    "# Make discriminator curves, Histogram\n",
    "\n",
    "# manipluate data so can be used for Histograms\n",
    "lst_bg_predicted = []\n",
    "for i in df_bg['Predicted']:\n",
    "    lst_bg_predicted.append(i)\n",
    "\n",
    "lst_of_values_bg_pred_train = [arr.item() for arr in lst_bg_predicted]\n",
    "\n",
    "lst_signal_predicted = []\n",
    "for i in df_signal['Predicted']:\n",
    "    lst_signal_predicted.append(i)\n",
    "\n",
    "lst_of_values_signal_pred_train = [arr.item() for arr in lst_signal_predicted]\n",
    "\n",
    "binwidth = 0.01\n",
    "bins=np.arange(0, 1 + binwidth, binwidth)\n",
    "    \n",
    "\n",
    "#Unweighted, normalised\n",
    "plt.hist(x= lst_of_values_bg_pred_train, bins=bins, density = True, histtype = 'step', label = 'Background')\n",
    "plt.hist(x= lst_of_values_signal_pred_train, bins=bins, density = True, histtype = 'step', label = 'Signal')\n",
    "plt.title('Training Discriminator Curve')\n",
    "plt.xlabel('Classifier Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Weighted to xs_weights x 300000, need to be toproportion of full dataset too\n",
    "#normalised\n",
    "\n",
    "plt.hist(x= lst_of_values_bg_pred_train, bins=bins,  weights = df_weights[:len(lst_of_values_bg_pred_train),1]*3e5*1/0.8,\n",
    "         histtype = 'step', label = 'Background')\n",
    "plt.hist(x= lst_of_values_signal_pred_train, bins=bins, weights = df_weights[:len(lst_of_values_signal_pred_train),3]*3e5*1/0.8,\n",
    "         histtype = 'step', label = 'Signal')\n",
    "plt.title('Training Discriminator Curve')\n",
    "plt.xlabel('Classifier Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0.8,1)    #zoom in on 0.9 to 1 region\n",
    "plt.ylim(0,200)    # need to zoom in\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#start dropping bins for signifucance part, calc Z and plot, should get curve that peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the validation data\n",
    "\n",
    "model = best_model \n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_valid, y_valid)   \n",
    "print(\"Validation loss:\", val_loss)\n",
    "print(\"Validation accuracy:\", val_acc)\n",
    "\n",
    "y_pred = model.predict(X_valid)\n",
    "y_pred_round = np.round(y_pred)\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_round, normalize = 'true')    #,'true' pred, all},)#\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Background', 'Signal']) # other env\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm)   \n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "print('Total signal processes;', cm[1,0]+cm[1,1])\n",
    "print('Total background processes;', cm[0,0]+cm[0,1])\n",
    "\n",
    "# can see no class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #using a permutation method to see what features are important\n",
    "\n",
    "# # Get the number of features\n",
    "# n_features = X_valid.shape[1]\n",
    "\n",
    "# # Initialize an array to store feature importances\n",
    "# importances = np.zeros(n_features)\n",
    "\n",
    "# # Calculate the feature importances using the permutation feature importance method\n",
    "# # Drops each variable one at a time \n",
    "# for i in range(n_features):\n",
    "#     print(i)    #counter to check is working, should go to 40\n",
    "#     feature_name = X_valid.columns[i]\n",
    "#     X_permuted = X_valid.copy().values\n",
    "#     X_permuted[:, i] = np.random.permutation(X_permuted[:, i])\n",
    "#     y_permuted = model.predict(X_permuted)\n",
    "#     importances[i] = np.abs(y_permuted - y_pred).mean()\n",
    "#     #importances[i] = (y_permuted - y_pred).mean()    #try without a abs, see if positively or negatively affected by removal?\n",
    "#     print(f\"Permuted feature {feature_name}\")\n",
    "\n",
    "# # Normalize the feature importances\n",
    "# importances /= importances.sum()   # importances sum to 1, so can be taken as a literal how important a variable\n",
    "# # is for the model\n",
    "\n",
    "# print(importances)  # using abs\n",
    "\n",
    "# print()#\n",
    "\n",
    "# print(np.sort(importances))\n",
    "\n",
    "# # np.random.permutation randomly permutes (rearranges in a random order) the elements.\n",
    "# # In this case, it is used to shuffle the values in a single column of the X_permuted array.\n",
    "# # Guess then this variable isnt useful in the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances_dict = dict(zip(importances, X_valid.columns))\n",
    "# # key:value\n",
    "\n",
    "# #print(combined_dict)\n",
    "\n",
    "# # get values for keys less than 0.01\n",
    "# values = []\n",
    "# for key, value in importances_dict.items():\n",
    "#     if key < 0.01:\n",
    "#         values.append(value)\n",
    "\n",
    "# # print the values\n",
    "# print(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of numbers less than 0.01\n",
    "# for i in range(len(importances)):\n",
    "#     if importances[i] < 0.05:\n",
    "#         print(X_valid.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if order of y_valid is the same order as y_pred\n",
    "# expect to be the ssame order if accuracy is same as validation accuracy above.\n",
    "\n",
    "# list of true outcomes\n",
    "y_valid_list = list(y_valid)\n",
    "\n",
    "# list of rounded predictions \n",
    "y_pred_round_list = list(y_pred_round)\n",
    "\n",
    "# list of whether these are same    \n",
    "z=[]\n",
    "for i in range(len(y_valid_list)):\n",
    "    if y_valid_list[i]==y_pred_round_list[i]:\n",
    "        z.append(1)\n",
    "    else:\n",
    "        z.append(0)\n",
    "        \n",
    "print(sum(z)/len(z))\n",
    "\n",
    "# Set a tolerance\n",
    "tol = 0.0001\n",
    "print(val_acc - tol < sum(z)/len(z) < val_acc + tol)\n",
    "\n",
    "# If true, suggests are in same order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(y_pred))\n",
    "max(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'True_Val': y_valid_list, 'Predicted': list(y_pred), 'Round Prediction': y_pred_round_list}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Easiest to reorder df, so all 0 (background) are first and then 1's (signal)\n",
    "\n",
    "#Verified, works\n",
    "df = df.sort_values(by='True_Val')\n",
    "df\n",
    "\n",
    "# Convert arrays to scalars\n",
    "df['Round Prediction'] = df['Round Prediction'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that they maintained order in the df:\n",
    "\n",
    "lst=[]\n",
    "for i in range(len(df)):\n",
    "    if df['True_Val'][i] == df['Round Prediction'][i]:\n",
    "        lst.append(1)\n",
    "    else:\n",
    "        (lst.append(0))\n",
    "print(sum(lst)/len(lst))\n",
    "\n",
    "# Set a tolerance\n",
    "tol = 0.0001\n",
    "print(val_acc - tol < sum(lst)/len(lst) < val_acc + tol)\n",
    "\n",
    "# If true, suggests are in same order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_one = df['Round Prediction'].value_counts()[1.0]\n",
    "ones_in_set = df['True_Val'].value_counts()[1]\n",
    "\n",
    "# Print the result\n",
    "print(pred_one)\n",
    "print('There is', ones_in_set, 'signal in the set')\n",
    "print('The model predicted',pred_one-ones_in_set ,'more ones than in reality')\n",
    "\n",
    "\n",
    "pred_zeros = df['Round Prediction'].value_counts()[0.0]\n",
    "zeros_in_set = df['True_Val'].value_counts()[0]\n",
    "\n",
    "# Print the result\n",
    "print(pred_zeros)\n",
    "print('There is', zeros_in_set, 'background in the set')   #agrees w/ confusion matrix\n",
    "print('The model predicted', pred_zeros-zeros_in_set,'more ones than in reality')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now first half of df is background, other half is signal\n",
    "\n",
    "PROBLEM, TEST AND VALID WAS SPLIT RANDOMLY, SO NOT ACTUALLY SAME AMOUNT OF 0's AND 1's\n",
    "\n",
    "Not a problem, but need to check if should be like this\n",
    "\n",
    " A couple of options, I could:\n",
    "\n",
    "Deal with it, and let there be uneven amounts of 1's and 0's\n",
    "\n",
    "OR\n",
    "\n",
    "Go back to test_train_split, and apply individually to signal and background and then sum them?\n",
    "\n",
    "This would guarantee 50/50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Discriminator curves\n",
    "\n",
    "#Unweighted, normalised\n",
    "plt.hist(x= lst_of_values_bg_pred_train, bins=bins, density = True, histtype = 'step', label = 'Background')\n",
    "plt.hist(x= lst_of_values_signal_pred_train, bins=bins, density = True, histtype = 'step', label = 'Signal')\n",
    "plt.title('TRAINING - Discriminator Curve')\n",
    "plt.xlabel('Classifier Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Weighted to xs_weights x 300000, need to be toproportion of full dataset too\n",
    "#normalised\n",
    "\n",
    "\n",
    "plt.hist(x= lst_of_values_bg_pred_train, bins=bins,  weights = df_weights[:len(lst_of_values_bg_pred_train),1]*3e5,\n",
    "         histtype = 'step', label = 'Background')\n",
    "plt.hist(x= lst_of_values_signal_pred_train, bins=bins, weights = df_weights[:len(lst_of_values_signal_pred_train),3]*3e5,\n",
    "         histtype = 'step', label = 'Signal')\n",
    "plt.title('TRAINING - Discriminator Curve')\n",
    "plt.xlabel('Classifier Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0.6,1)    #zoom in on 0.9 to 1 region\n",
    "plt.ylim(0,500)    # need to zoom in\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split df into signal and bg\n",
    "\n",
    "df_bg = df[df['True_Val'] == 0]\n",
    "df_signal = df[df['True_Val'] == 1]\n",
    "\n",
    "# Make Validation discriminator curves, Histogram\n",
    "\n",
    "# manipluate data so can be used for Histograms\n",
    "lst_bg_predicted = []\n",
    "for i in df_bg['Predicted']:\n",
    "    lst_bg_predicted.append(i)\n",
    "\n",
    "lst_of_values_bg_pred = [arr.item() for arr in lst_bg_predicted]\n",
    "\n",
    "lst_signal_predicted = []\n",
    "for i in df_signal['Predicted']:\n",
    "    lst_signal_predicted.append(i)\n",
    "\n",
    "lst_of_values_signal_pred = [arr.item() for arr in lst_signal_predicted]\n",
    "\n",
    "binwidth = 0.01\n",
    "bins=np.arange(0, 1 + binwidth, binwidth)\n",
    "\n",
    "#Unweighted, normalised\n",
    "plt.hist(x= lst_of_values_bg_pred, bins=bins, density = True, histtype = 'step', label = 'Background')\n",
    "plt.hist(x= lst_of_values_signal_pred, bins=bins, density = True, histtype = 'step', label = 'Signal')\n",
    "plt.title('Discriminator Curve')\n",
    "plt.xlabel('Classifier Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Weighted to xs_weights x 300000, need to be toproportion of full dataset too\n",
    "#normalised\n",
    "plt.hist(x= lst_of_values_bg_pred, bins=bins,  weights = df_weights[:len(lst_of_values_bg_pred),1]*3e5,\n",
    "         histtype = 'step', label = 'Background')\n",
    "plt.hist(x= lst_of_values_signal_pred, bins=bins, weights = df_weights[:len(lst_of_values_signal_pred),3]*3e5,\n",
    "         histtype = 'step', label = 'Signal')\n",
    "plt.title('Discriminator Curve')\n",
    "plt.xlabel('Classifier Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "#plt.xlim(0.9,1)    #zoom in on 0.9 to 1 region\n",
    "#plt.ylim(0,200)    # need to zoom in\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "plt.hist(x= lst_of_values_bg_pred, bins=bins,  weights = df_weights[:len(lst_of_values_bg_pred),1]*3e5,\n",
    "         histtype = 'step', label = 'Background')\n",
    "plt.hist(x= lst_of_values_signal_pred, bins=bins, weights = df_weights[:len(lst_of_values_signal_pred),3]*3e5,\n",
    "         histtype = 'step', label = 'Signal')\n",
    "plt.title('Discriminator Curve')\n",
    "plt.xlabel('Classifier Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(0.8,1)    #zoom in on 0.9 to 1 region\n",
    "plt.ylim(0,200)    # need to zoom in\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#start dropping bins for signifucance part, calc Z and plot, should get curve that peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and Training overlayed\n",
    "\n",
    "#Unweighted, normalised\n",
    "plt.hist(x= lst_of_values_bg_pred, bins=bins, density = True, histtype = 'step', label = 'Background_test')\n",
    "plt.hist(x= lst_of_values_signal_pred, bins=bins, density = True, histtype = 'step', label = 'Signal_test')\n",
    "plt.title('Test - Discriminator Curve')\n",
    "plt.xlabel('Classifier Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "#Unweighted, normalised\n",
    "plt.hist(x= lst_of_values_bg_pred_train, bins=bins, density = True, histtype = 'step', label = 'Background_train')\n",
    "plt.hist(x= lst_of_values_signal_pred_train, bins=bins, density = True, histtype = 'step', label = 'Signal_train')\n",
    "plt.title('Mix Discriminator Curve')\n",
    "plt.xlabel('Classifier Prediction')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate number of events\n",
    "\n",
    "n_bg, bins_bg, patches_bg = plt.hist(x= lst_of_values_bg_pred, bins=bins, weights = df_weights[:len(lst_of_values_bg_pred),1]*3e5*5,\n",
    "         histtype = 'step', label = 'Background')\n",
    "n_signal, bins_signal, patches_signal = plt.hist(x= lst_of_values_signal_pred, bins=bins, weights = df_weights[:len(lst_of_values_signal_pred),3]*3e5*5,\n",
    "         histtype = 'step', label = 'Signal')\n",
    "\n",
    "# calculate the number of events\n",
    "num_bg_events = np.sum(n_bg)\n",
    "num_signal_events = np.sum(n_signal)\n",
    "\n",
    "print('num_bg_events = ', num_bg_events)\n",
    "print('num_signal_events = ', num_signal_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot ROC curves\n",
    "\n",
    "# y_true = df['True_Val']\n",
    "# y_score = df['Predicted']\n",
    "\n",
    "# # calculate the false positive rate and true positive rate\n",
    "# fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "\n",
    "# # calculate the area under the ROC curve\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# # plot the ROC curve\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "\n",
    "# print(roc_auc)  #79%\n",
    "\n",
    "# print(len(thresholds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_roc = {'thresholds': thresholds, 'tpr': tpr, 'fpr': fpr, 'tpr/sqrt(fpr)':tpr/np.sqrt(fpr)}\n",
    "# df_roc = pd.DataFrame(data_roc)\n",
    "# #df_roc\n",
    "# df_roc.head(20)\n",
    "# plt.plot(df_roc['thresholds'][3:], df_roc['tpr/sqrt(fpr)'][3:])\n",
    "# plt.show()\n",
    "# max(df_roc['tpr/sqrt(fpr)'][3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot precision recall curve\n",
    "# from sklearn.metrics import average_precision_score\n",
    "\n",
    "# y_true = df['True_Val']\n",
    "# y_score = df['Predicted']\n",
    "\n",
    "# # calculate the precision-recall curve\n",
    "# precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "# # plot the precision-recall curve\n",
    "# plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "# plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.title('Precision-Recall Curve')\n",
    "# plt.show()\n",
    "\n",
    "# AUC = average_precision_score(y_true, y_score)   #0.793, pretty similar to roc AUC\n",
    "# print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['True_Val']\n",
    "x = df['Predicted']\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significance\n",
    "\n",
    "$$ Z = \\frac{s}{\\sqrt(B+\\sigma_s^{2})}  $$\n",
    "\n",
    "s = number of signal\n",
    "\n",
    "B = number of background\n",
    "\n",
    "$\\sigma_s$ = statistical uncertainty (~10% of B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bg_events = np.sum(n_bg)\n",
    "num_signal_events = np.sum(n_signal)\n",
    "\n",
    "print(num_bg_events)\n",
    "print(num_signal_events)\n",
    "\n",
    "z = np.empty(len(bins_bg))\n",
    "for i in range(len(bins_bg)):\n",
    "    num_bg_events = np.sum(n_bg[i:])\n",
    "    num_signal_events = np.sum(n_signal[i:])\n",
    "    #print(num_signal_events)\n",
    "    z_i = num_signal_events / np.sqrt(num_bg_events)\n",
    "    z[i] = z_i\n",
    "#print(z)\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,len(z))\n",
    "plt.plot(bins_bg,z[:])\n",
    "#plt.xlim([0, 1.0])\n",
    "#plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Significance, z')\n",
    "plt.title('Significance curve with no sigma')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mask = z < 1e10     #just so doesnt include infs\n",
    "index_of_max = np.argmax(z[mask])\n",
    "max_value = np.amax(z[mask])\n",
    "\n",
    "\n",
    "#index_of_max = np.nanargmax(z[:])\n",
    "#max_value = np.nanmax(z[:])     # 1 point before last, should probs generalise so that can deal w inf etc \n",
    "max_bin = bins_bg[index_of_max]\n",
    "print(max_bin)\n",
    "print(index_of_max)\n",
    "print(max_value)\n",
    "\n",
    "# significance = sigmna, 6.5 signma, 5 sigma\n",
    "\n",
    "num_bg_threshold = np.sum(n_bg[index_of_max:])\n",
    "num_signal_threshold = np.sum(n_signal[index_of_max:])\n",
    "\n",
    "print()\n",
    "print('If cut at a threshold of', index_of_max/100, 'have')\n",
    "print(num_signal_threshold, 'singal events')\n",
    "print(num_bg_threshold, 'background events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matricies with new threshold at max significance\n",
    "\n",
    "# calculates prediction values\n",
    "#y_pred = model.predict(X_valid) \n",
    "\n",
    "significance_threshold = max_bin\n",
    "y_pred_threshold = np.where(y_pred >= significance_threshold, 1, 0)\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_threshold, normalize = 'true')    #,'true' pred, all},)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm)   \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Background', 'Signal']) # other env\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "print('Total signal processes;', cm[1,0]+cm[1,1])\n",
    "print('Total background processes;', cm[0,0]+cm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance with statistical uncertainty term\n",
    "\n",
    "num_bg_events = np.sum(n_bg)\n",
    "num_signal_events = np.sum(n_signal)\n",
    "\n",
    "print(num_bg_events)\n",
    "print(num_signal_events)\n",
    "\n",
    "z = np.empty(len(bins_bg))\n",
    "for i in range(len(bins_bg)):\n",
    "    num_bg_events = np.sum(n_bg[i:])\n",
    "    num_signal_events = np.sum(n_signal[i:])\n",
    "    #print(num_signal_events)\n",
    "    z_i = num_signal_events / np.sqrt(num_bg_events + (0.05*num_bg_events)**2)\n",
    "    \n",
    "    z[i] = z_i\n",
    "#print(z)\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,len(z))\n",
    "plt.plot(bins_bg,z[:])\n",
    "#plt.xlim([0, 1.0])\n",
    "#plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Significance, z')\n",
    "plt.title('Significance curve with sigma')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "mask = z < 1e10     #just so doesnt include infs\n",
    "index_of_max = np.argmax(z[mask])\n",
    "max_value = np.amax(z[mask])\n",
    "\n",
    "\n",
    "#index_of_max = np.nanargmax(z[:])\n",
    "#max_value = np.nanmax(z[:])     # 1 point before last, should probs generalise so that can deal w inf etc \n",
    "max_bin = bins_bg[index_of_max]\n",
    "print(max_bin)\n",
    "print(index_of_max)\n",
    "print(max_value)\n",
    "\n",
    "num_bg_threshold = np.sum(n_bg[index_of_max:])\n",
    "num_signal_threshold = np.sum(n_signal[index_of_max:])\n",
    "print()\n",
    "print('If cut at a threshold of', index_of_max/100, 'have')\n",
    "print(num_signal_threshold, 'singal events')\n",
    "print(num_bg_threshold, 'background events')\n",
    "# significance = sigmna, 6.5 signma, 5 sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matricies with new threshold at max significance with statisitcal uncertainty\n",
    "\n",
    "# calculates prediction values\n",
    "#y_pred = model.predict(X_valid) \n",
    "\n",
    "significance_threshold = max_bin\n",
    "y_pred_threshold = np.where(y_pred >= significance_threshold, 1, 0)\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_valid, y_pred_threshold, normalize = 'true')    #,'true' pred, all},)\n",
    "#disp = ConfusionMatrixDisplay(confusion_matrix=cm)   \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Background', 'Signal']) # other env\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "print('Total signal processes;', cm[1,0]+cm[1,1])\n",
    "print('Total background processes;', cm[0,0]+cm[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See weight is const basically, so just use a value\n",
    "\n",
    "semi_xs_weight = df2['xs_weight']   # plot to show is wrong, at least 1 -ive val (is this right?)\n",
    "signal_xs_weight = df4['xs_weight'] \n",
    "#plt.hist(x=semi_xs_weight, bins = 10, weights = df2.xs_weight, density = True, histtype = 'step', label = 'Background')\n",
    "plt.hist(x=signal_xs_weight, bins = 10, weights = df4.xs_weight, density = True, histtype = 'step', label = 'Signal')\n",
    "plt.title('xs_weight')\n",
    "plt.legend()\n",
    "plt.show() \n",
    "\n",
    "semi_xs_weight = df2['xs_weight']   # plot to show is wrong, at least 1 -ive val (is this right?)\n",
    "signal_xs_weight = df4['xs_weight'] \n",
    "plt.hist(x=semi_xs_weight, bins = 10, weights = df2.xs_weight, density = True, histtype = 'step', label = 'Background')\n",
    "#plt.hist(x=signal_xs_weight, bins = 10, weights = df4.xs_weight, density = True, histtype = 'step', label = 'Signal')\n",
    "plt.title('xs_weight')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also, to make sure we get the event weights correct, can each of you try and produce a number for the \n",
    "#total number of events you obtain when using all events in the ttH125 and ttbar semileptonic dataframes,\n",
    "#normalized to 300/fb?\n",
    "#Hopefully you all arrive at the same result.\n",
    "\n",
    "# Sligthly confused, does x_s weight normalised already entail no. events,\n",
    "# So xs_weight * 300,000 is no events\n",
    "\n",
    "#Or\n",
    "\n",
    "# Are we meant to use the equation you showed us last week and work out what N should be\n",
    "\n",
    "#Or\n",
    "\n",
    "# do both and compare them as should be same?\n",
    "\n",
    "bg_weight = semi_xs_weight[1]\n",
    "signal_weight = signal_xs_weight[1]\n",
    "\n",
    "# total number SL\n",
    "sl_tot_events = bg_weight * 300000      # weight per pb to 300fb\n",
    "\n",
    "# total number signal\n",
    "signal_tot_events = signal_weight * 300000\n",
    "\n",
    "print('Total number of semileptonic events is {}'.format(sl_tot_events))\n",
    "print('Total number of ttH125 events is {}'.format(signal_tot_events))\n",
    "print()\n",
    "\n",
    "sl_events = 300000 * 500\n",
    "signal_events = 300000 * 0.5\n",
    "\n",
    "print('Total number of semileptonic events is {}'.format(sl_events)) #150000k\n",
    "print('Total number of ttH125 events is {}'.format(signal_events))   #150k\n",
    "#tot_events = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q \n",
    "Can you also overlay the discriminator distributions for the test data \n",
    "(normalized to the same area) to check if training and testing samples agree?\n",
    "# A\n",
    "not sure what is meant, so show code for the one that I did do. I did it for validation data, so when he says this does hr mean my data is test or training\n",
    "\n",
    "# Q \n",
    "If you use all the events in the dataframes and weight each entry by xs_weight you should\n",
    "get the expected number of events for 1 /pb. Is everything in the plot below already \n",
    "multiplied by 300000 to get the event yields expected for 300/fb?\n",
    "What exactly did you use?\n",
    "\n",
    "# A\n",
    "Yes, everything in the weighted plot is multiplied by 300000, show what I used as I am not sure if thats right\n",
    "\n",
    "# Q\n",
    "If you integrate the weighted histograms, how many events do you obtain in each histogram? \n",
    "\n",
    "# A\n",
    "Bit confused, why integrate the histograms when can just see how much data used, is it because this is weighted so it will give the number of events we actually expect overall in 300000 fb^-1, makes sense.\n",
    "#Did 3 ways, not really clear what was right, way I think rn is each bin is a number of events, so just need to add number events per bin, dont need to do anything regarding the width of the bin? But then an integral is the area so that does mean bin size matters, but not really sure, bc obvs sum bin size = 1\n",
    "#Think first one is correct\n",
    "\n",
    "\n",
    "# Q \n",
    "Next would be to plot ROC curves and precision-recall which have slightly different meaning.\n",
    "\n",
    "# A\n",
    "So does a ROC curve work out where the best threshold is for us, so what is significance. Also should a ROC curve be weighted\n",
    "\n",
    "# Q \n",
    "from the weighted plot, make a plot of Z = S/sqrt(B) as a function of the cut on the discriminator value, where S and B are the number of events (i.e. the integral) of the histogram \n",
    "that have discriminator value above the cut you chose. You may need to zoom in on the region >0.9 \n",
    "to see that it falls off again as you approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1['xs_weight'])\n",
    "plt.hist(df1['xs_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2['xs_weight'])\n",
    "plt.hist(df2['xs_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3['xs_weight'])\n",
    "plt.hist(df3['xs_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df4['xs_weight'])\n",
    "plt.hist(df4['xs_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TTToHadronic weight: ', df1['xs_weight'][10000])\n",
    "print('TTToSL weight: ', df2['xs_weight'][0])\n",
    "print('TTL2NU weight: ', df3['xs_weight'][0])\n",
    "print('TTH125 weight: ', df4['xs_weight'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates from 08/02 grind\n",
    "- I trained on full dataset and got like 10% more accuracy, actually a v good now, loss also down a fair bit, 0.1\n",
    "\n",
    "- But does take like 10 mins to run training data\n",
    "\n",
    "- Actually, not that good\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
