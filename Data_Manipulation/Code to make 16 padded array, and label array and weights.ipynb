{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras.utils import to_categorical ## specific  one I wanna use\n",
    "from tensorflow.keras import utils\n",
    "#from tensorflow.keras.layers import Input, Dense, Dropout  # Specific ones I wanna use, can just import whole module\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.models import Model\n",
    "import h5py\n",
    "import tables\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=1\n",
    "y=3\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "df\n"
     ]
    }
   ],
   "source": [
    "# Import data from all 4 collision types.\n",
    "\n",
    "input_file_1 = \"/scratch/ys20884/ml_vars/TTToHadronic/df_ml_inputs.hd5\"\n",
    "\n",
    "with h5py.File(input_file_1, \"r\") as f:\n",
    "    #print(\"Contents of the file:\")\n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "\n",
    "df1= pd.read_hdf(input_file_1, key=\"df\")\n",
    "\n",
    "\n",
    "input_file2 = \"/scratch/ys20884/ml_vars/TTToSemiLeptonic/df_ml_inputs.hd5\"\n",
    "\n",
    "with h5py.File(input_file2, \"r\") as f:\n",
    "    #print(\"Contents of the file:\")\n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "\n",
    "df2 = pd.read_hdf(input_file2, key=\"df\")\n",
    "print(\"Dataframe columns:\")\n",
    "print(df2.columns)\n",
    "\n",
    "input_file3 = \"/scratch/ys20884/ml_vars/TTTo2L2Nu/df_ml_inputs.hd5\"\n",
    "\n",
    "with h5py.File(input_file3, \"r\") as f:\n",
    "    #print(\"Contents of the file:\")\n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "\n",
    "df3 = pd.read_hdf(input_file3, key=\"df\")\n",
    "\n",
    "input_file4 = \"/scratch/ys20884/ml_vars/ttH125/df_ml_inputs.hd5\"\n",
    "\n",
    "with h5py.File(input_file4, \"r\") as f:\n",
    "    #print(\"Contents of the file:\")\n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "\n",
    "df4 = pd.read_hdf(input_file4, key=\"df\")\n",
    "\n",
    "print(len(df1))\n",
    "print(len(df2))\n",
    "print(len(df3))\n",
    "print(len(df4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one big dataframe, full data set long to practice with\n",
    "df_total = df2.append(df1)\n",
    "df_total = df_total.append(df3)\n",
    "df_total = df_total.append(df4)\n",
    "print(len(df_total))   \n",
    "\n",
    "# Background is 0\n",
    "# Signal is 1\n",
    "df_process = df_total['dataset']\n",
    "df_total['dataset'] = df_total['dataset'].replace({'ttH125': 1, 'TTToSemiLeptonic': 0, 'TTToHadronic': 0, 'TTTo2L2Nu':0})\n",
    "df_total.head()\n",
    "# Have done test cases, is working fine.\n",
    "print(len(df_total))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all labels and the phi's\n",
    "df_total.drop(['entry', 'BiasedDPhi', 'hashed_filename', 'weight_nominal', 'xs_weight', \n",
    "              'InputMet_phi', 'MHT_phi', 'boostedObject_phi', 'boostedTop_phi', 'boostedV_phi',\n",
    "              'cleanedBJet_phi','cleanedJet_phi'], axis=1, inplace=True)#\n",
    "df_total.columns            # Said BiasedDPhi looks wrong\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for column in df_total:\n",
    "    if df_total[column].dtype == 'object':   \n",
    "        print(column, 'is a jet based variable')\n",
    "        lst.append(column)\n",
    "    elif column == 'dataset':    # dont want to drop dataset column (label)\n",
    "        print('Need to keep labels')\n",
    "        lst.append(column)\n",
    "    else:\n",
    "        df_total.drop([column], axis=1, inplace=True)\n",
    "print(len(lst)-1, 'variables are left')    # -1 as - label\n",
    "df_total.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to find the longest array in any column for my first method\n",
    "\n",
    "# for column in df_total:\n",
    "#     for i in column:\n",
    "#         df_total[column]\n",
    "#    x_arr = np.max(df_total[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each column and find the maximum array length\n",
    "max_lengths = {}\n",
    "min_lengths = {}\n",
    "for column in df_total:\n",
    "    if df_total[column].dtype == 'object': \n",
    "        #print(column)\n",
    "        max_lengths[column] = max([len(arr) for arr in df_total[column]])\n",
    "        min_lengths[column] = min([len(arr) for arr in df_total[column]])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print('max lengths=', max_lengths)\n",
    "print('min lenghts =', min_lengths)\n",
    "arr_lengths = max(max_lengths.values())\n",
    "print('Need to pad to', arr_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for column in df_total: \n",
    "    if df_total[column].dtype == 'object': \n",
    "        i+=1\n",
    "        print(i)\n",
    "        df_total[column] = df_total[column].apply(lambda x: np.pad(x, (0, arr_lengths-len(x)), mode='constant'))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "df_total.drop(['dataset'], axis=1, inplace=True) # so only padded data, no labels    \n",
    "#df_total['boostedObject_area'] = df_total['boostedObject_area'].apply(lambda x: np.pad(x, (0, arr_lengths-len(x)), mode='constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape into 3D array, instead of 2D df\n",
    "padded_arr_3d = np.array([row.tolist() for row in df_total.values]).reshape(len(df_total), arr_lengths, df_total.shape[1])\n",
    "# only takes like 17 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to HDF5 file\n",
    "with h5py.File('padded_arr_3d.h5', 'w') as f:\n",
    "    f.create_dataset('array', data=padded_arr_3d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = df2['dataset'].append(df1['dataset'])\n",
    "df_labels = df_labels.append(df3['dataset'])\n",
    "df_labels = df_labels.append(df4['dataset'])\n",
    "\n",
    "df_labels = df_labels.replace({'ttH125': 1, 'TTToSemiLeptonic': 0, 'TTToHadronic': 0, 'TTTo2L2Nu':0})\n",
    "\n",
    "\n",
    "# Save to HDF5 file\n",
    "with h5py.File('4_event_types_labels.h5', 'w') as f:\n",
    "    f.create_dataset('array', data=df_labels)\n",
    "\n",
    "# This became really complicated for some reason, and isn't more efficient to a point worth working out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load padded data\n",
    "#with h5py.File('padded_arr_3d.h5', 'r') as f:\n",
    " #   padded_arr_3d = f['array'][:]\n",
    "#padded_arr_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weights = pd.concat([df1['xs_weight'], df2['xs_weight'], df3['xs_weight'], df4['xs_weight']], axis=1)\n",
    "df_weights.columns = ['df1_xs_weight', 'df2_xs_weight', 'df3_xs_weight', 'df4_xs_weight']\n",
    "(df_weights)\n",
    "\n",
    "with h5py.File('4_event_types_weights.h5', 'w') as f:\n",
    "    f.create_dataset('array', data=df_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = df2['dataset'].append(df1['dataset'])\n",
    "df_labels = df_labels.append(df3['dataset'])\n",
    "df_labels = df_labels.append(df4['dataset'])\n",
    "\n",
    "df_labels = df_labels.replace({'ttH125': 1, 'TTToSemiLeptonic': 0, 'TTToHadronic': 0, 'TTTo2L2Nu':0})\n",
    "\n",
    "\n",
    "# Save to HDF5 file\n",
    "with h5py.File('4_event_types_labels.h5', 'w') as f:\n",
    "    f.create_dataset('array', data=df_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('trained_model.h5')\n",
    "\n",
    "#from tensorflow.keras.models import load_model\n",
    "#loaded_model = load_model('trained_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
